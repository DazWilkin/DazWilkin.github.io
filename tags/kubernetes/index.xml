<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on (p)retired</title>
    <link>https://pretired.dazwilkin.com/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on (p)retired</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Jun 2022 00:00:00 -0700</lastBuildDate><atom:link href="https://pretired.dazwilkin.com/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Secure (TLS) gRPC services with VKE</title>
      <link>https://pretired.dazwilkin.com/posts/220603/</link>
      <pubDate>Fri, 03 Jun 2022 00:00:00 -0700</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/220603/</guid>
      <description>NOTE cert-manager is a better solution to what follows.
 I&amp;rsquo;ve a need to deploy a Vultr Kubernetes Engine (VKE) cluster on a daily basis (create and delete within a few hours) and expose (securely|TLS) a gRPC service.
I have an existing solution Automatic Certs w/ Golang gRPC service on Compute Engine that combines a gRPC Healthchecking and an ACME service and decided to reuse this.
In order for it work, we need:</description>
    </item>
    
    <item>
      <title>Vultr CLI and JSON output</title>
      <link>https://pretired.dazwilkin.com/posts/220602/</link>
      <pubDate>Thu, 02 Jun 2022 00:00:00 -0700</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/220602/</guid>
      <description>I&amp;rsquo;ve begun exploring Vultr after the company announced a managed Kubernetes offering Vultr Kubernetes Engine (VKE).
In my brief experience, it&amp;rsquo;s a decent platform and its CLI vultr-cli is mostly (!) good. The CLI has a limitation in that command output is text formatted and this makes it challenging to parse the output when scripting.
 NOTE The Vultr developers have a branch rewrite that includes a solution to this problem.</description>
    </item>
    
    <item>
      <title>Prometheus VPA Recommendations</title>
      <link>https://pretired.dazwilkin.com/posts/210305/</link>
      <pubDate>Fri, 05 Mar 2021 00:00:00 -0800</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/210305/</guid>
      <description>Phew!
For Want of a Nail
I was interested in learning how to Manage Resources for Containers. On the way, I learned and discovered:
 kubectl top Vertical Pod Autoscaler A (valuable) digression through PodMonitor kube-state-metrics `kubectl-patch Created a Graph References  Kubernetes Resources Visual Studio Code has begun to bug me (reasonably) to add resources to Kubernetes manifests.
E.g.:
resources:  limits:  cpu: &amp;#34;1&amp;#34;  memory: &amp;#34;512Mi&amp;#34; I&amp;rsquo;ve been spending time with Deislab&amp;rsquo;s Akri and decided to determine whether Akri&amp;rsquo;s primary resources (Agent, Controller) and some of my creations HTTP Device and Discovery, were being suitably constrained.</description>
    </item>
    
    <item>
      <title>Krustlet on DO Managed Kubernetes</title>
      <link>https://pretired.dazwilkin.com/posts/210122/</link>
      <pubDate>Fri, 22 Jan 2021 00:00:00 -0800</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/210122/</guid>
      <description>I&amp;rsquo;ve spent time this week returning to the interesting Deislabs project Krustlet. Since the last time, the bootstrapping process has been simplified using Kubernetes Bootstrap Tokens. I know this updated process works with MicroK8s. Unfortunately, I&amp;rsquo;m struggling with it on GKE and thought I&amp;rsquo;d try DigitalOcean Managed Kubernetes.
It worked first time!
In the following, we run both the Kubernetes cluster and the Krustlet Droplet on DigitalOcean but, as long as the cluster and the VM are able to communicate with one another, you should be able to run these anywhere.</description>
    </item>
    
    <item>
      <title>Kubernetes cert-manager</title>
      <link>https://pretired.dazwilkin.com/posts/210108/</link>
      <pubDate>Fri, 08 Jan 2021 00:00:00 -0800</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/210108/</guid>
      <description>I developed an admission webhook for Akri, twice (Golang, Rust). I naively followed other examples for the generation of the certificates, created a 1.20 cluster and broke that process.
I&amp;rsquo;d briefly considered using cert-manager recently but quickly abandoned the idea thinking it would be onerous and unnecessary complexity for little-old-me. I was wrong. It&amp;rsquo;s excellent and I recommend it highly.
I won&amp;rsquo;t reproduce the v1beta1 and v1 examples from the Stackoverflow question as they should be self-explanatory.</description>
    </item>
    
    <item>
      <title>Kubernetes Webhooks</title>
      <link>https://pretired.dazwilkin.com/posts/201229/</link>
      <pubDate>Tue, 29 Dec 2020 00:00:00 -0800</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/201229/</guid>
      <description>I spent some time last week writing my first admission webhook for Kubernetes. I wrote the handler in Golang because I&amp;rsquo;m most familiar with Golang and because, as Kubernetes&amp;rsquo; native language, I was more confident that the necessary SDKs would exist and that the documentation would likely use Golang by default. I struggled to find useful documentation and so this post is to help you (and me!) remember how to do this next time!</description>
    </item>
    
    <item>
      <title>Kubernetes Device Plugins</title>
      <link>https://pretired.dazwilkin.com/posts/201217/</link>
      <pubDate>Thu, 17 Dec 2020 00:00:00 -0800</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/201217/</guid>
      <description>I&amp;rsquo;m debugging an issue with Akri Zeroconf protocol in which Instance environment variables are no longer (!) being surfaced within the Broker pods. In my adventures, it seemed useful to better understand how Akri works and specifically, how Akri uses Kubernetes Device Plugins.
IIUC plugins register with the Kubelet (!) via a gRPC service (Registration) that the Kubelet exposes on a UNIX socket at /var/lib/kubelet/device-plugins/kubelet.sock
Then (!) if successful, devices should be reported by the Node&amp;rsquo;s metadata (spec) and available to be bound to Pods.</description>
    </item>
    
    <item>
      <title>Akri</title>
      <link>https://pretired.dazwilkin.com/posts/201113/</link>
      <pubDate>Fri, 13 Nov 2020 00:00:00 -0800</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/201113/</guid>
      <description>For the past couple of weeks, I&amp;rsquo;ve been playing around with Akri, a Microsoft (DeisLabs) project for building a connected edge with Kubernetes. Kubernetes, IoT, Rust (and Golang) make this all compelling to me.
Initially, I deployed an Akri End-to-End to MicroK8s on Google Compute Engine (link) and Digital Ocean (link). But I was interested to create me own example and so have proposed a very (!) simple HTTP-based protocol.</description>
    </item>
    
    <item>
      <title>akri</title>
      <link>https://pretired.dazwilkin.com/posts/201022/</link>
      <pubDate>Thu, 22 Oct 2020 00:00:00 -0700</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/201022/</guid>
      <description>I was very interested to read about Microsoft&amp;rsquo;s DeisLab&amp;rsquo;s latest (rust-based) Kubernetes project: akri. If I understand it correctly, it provides a mechanism to make any (IoT) device accessible to containers running within a cluster. I need to spend more time playing around with it so that I can fully understand it. I had some problems getting the End-to-End demo running on Google Compute Engine (and then I tried DigitalOcean droplet) instances.</description>
    </item>
    
    <item>
      <title>Accessing GCR repos from Kubernetes</title>
      <link>https://pretired.dazwilkin.com/posts/200207/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 -0800</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/200207/</guid>
      <description>Until today, I&amp;rsquo;d not accessed a Google Container Registry repo from a non-GKE Kubernetes deployment.
It turns out that it&amp;rsquo;s pretty well-documented (link) but, here&amp;rsquo;s an end-end example.
Assuming:
BILLING=[[YOUR-BILLING]] PROJECT=[[YOUR-PROJECT]] SERVER=&amp;#34;us.gcr.io&amp;#34; If not already:
gcloud projects create {$PROJECT}  gcloud beta billing projects link ${PROJECT} \ --billing-account=${BILLING}  gcloud services enable containerregistry.googleapis.com \ --project=${PROJECT} Container Registry IMAGE=&amp;#34;busybox&amp;#34; # Or ...  docker pull ${IMAGE}  docker tag \  ${IMAGE} \  ${SERVER}/${PROJECT}/${IMAGE}  docker push ${SERVER}/${PROJECT}/${IMAGE}  gcloud container images list-tags ${SERVER}/${PROJECT}/${IMAGE} Service Account Create a service account that&amp;rsquo;s permitted to download (read-only) images from this project&amp;rsquo;s registry</description>
    </item>
    
    <item>
      <title>NGINX Ingress</title>
      <link>https://pretired.dazwilkin.com/posts/191211/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 -0700</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/191211/</guid>
      <description>I&amp;rsquo;ve written a couple of deployment options (Google Compute Engine; Kubernetes) for an open-source project. The Kubernetes deployment provides NodePort and (TCP) LoadBalancer options and I&amp;rsquo;ve been trying (unsuccessfully) to add HTTPS Load-balancing.
I should (!) try to deploy to Google Kubernetes Engine (GKE) but I&amp;rsquo;ve been using microk8s, Digital Ocean Managed Kubernetes and the Linode LKE Beta. Each of these requires an implementation of Ingress controller. For GKE, GCP&amp;rsquo;s HTTP/S Load-balancer (GCLB) is used.</description>
    </item>
    
    <item>
      <title>Kubernetes Engine and Free Tier</title>
      <link>https://pretired.dazwilkin.com/posts/190924/</link>
      <pubDate>Tue, 24 Sep 2019 09:40:14 -0700</pubDate>
      
      <guid>https://pretired.dazwilkin.com/posts/190924/</guid>
      <description>Google Cloud Platform Free Tier appears (please verify this for yourself) to provide the ability to run a(n admittedly miniscule) Kubernetes cluster for free. So, why do this? It provides a definitive Kubernetes (Engine) experience on Google Cloud Platform that you may use for learning and testing.
Kubernetes Engine the master node(s) and the control plane are free.
Kubernetes (i.e. Compute Engine) nodes potentially incur charges including for the VM runtime and any attached storage, snapshots etc.</description>
    </item>
    
  </channel>
</rss>
